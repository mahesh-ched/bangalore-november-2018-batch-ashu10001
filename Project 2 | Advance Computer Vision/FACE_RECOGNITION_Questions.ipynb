{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZPrUBuA2D9n"
   },
   "source": [
    "## Deep face recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jpaCy9Q1y4M"
   },
   "source": [
    "### First, lets install the required libraries. Upload the `requirements.txt` file given and run the below commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ashishsingh/AIML/Assignments/Project 2 | Advance Computer Vision'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = \"/Users/ashishsingh/AIML/Assignments/Project 2 | Advance Computer Vision/files_required_for_face_recognition/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9k1hVvWLnGw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.1.10 (from -r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/b8/3dafc45f20a817ab9f042302646bcbe6f7e26e8a760871a85637e53a35ec/absl-py-0.1.10.tar.gz (79kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: appnope==0.1.0 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.0)\n",
      "Collecting bleach==1.5.0 (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler==0.10.0 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Collecting decorator==4.2.1 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/5a/53db15bf367d2028bdc6700dbdf1bdfab46b9f208b7516952817c0808118/decorator-4.2.1-py2.py3-none-any.whl\n",
      "Collecting entrypoints==0.2.3 (from -r requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/8b/4eefa9b47f1910b3d2081da67726b066e379b04ca897acfe9f92bac56147/entrypoints-0.2.3-py2.py3-none-any.whl\n",
      "Collecting h5py==2.7.1 (from -r requirements.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/7a/6048de44c62fc5e618178ef9888850c3773a9e4be249e5e673ebce0402ff/h5py-2.7.1.tar.gz (264kB)\n",
      "\u001b[K    100% |████████████████████████████████| 266kB 4.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from -r requirements.txt (line 8))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 5.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ipykernel==4.8.0 (from -r requirements.txt (line 9))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ea/73a743bc29672bcc9e3470317c1b20fe02f3ec2a275737f7c84ad7e96d1a/ipykernel-4.8.0-py3-none-any.whl (108kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 3.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ipython==6.2.1 (from -r requirements.txt (line 10))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/87/294b718125085559b56453be87d90777863173470167e5f1d5de20b9eea3/ipython-6.2.1-py3-none-any.whl (745kB)\n",
      "\u001b[K    100% |████████████████████████████████| 747kB 4.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (0.2.0)\n",
      "Collecting ipywidgets==7.1.1 (from -r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/af/1c31a69aa596294abbc6fabec049ee80332e9afe9fa05a1566d2905202f6/ipywidgets-7.1.1-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 13.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jedi==0.11.1 (from -r requirements.txt (line 13))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ca/d71f5a427601c98eadabfd73104cacbec8cc230e8416158decf61a48b0c6/jedi-0.11.1-py2.py3-none-any.whl (250kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 5.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Jinja2==2.10 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (2.10)\n",
      "Collecting jsonschema==2.6.0 (from -r requirements.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/de/47e35a97b2b05c2fadbec67d44cfcdcd09b8086951b331d82de90d2912da/jsonschema-2.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jupyter==1.0.0 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 16)) (1.0.0)\n",
      "Collecting jupyter-client==5.2.2 (from -r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/8a/5cca4d98fe28c4db6c2879b69ca5d106e0f4415aafda8bc4074deddbc3da/jupyter_client-5.2.2-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 6.4MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting jupyter-console==5.2.0 (from -r requirements.txt (line 18))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/82/6469cd7fccf7958cbe5dce2e623f1e3c5e27f1bb1ad36d90519bc2d5d370/jupyter_console-5.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jupyter-core==4.4.0 in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 19)) (4.4.0)\n",
      "Collecting Keras==2.1.3 (from -r requirements.txt (line 20))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting Markdown==2.6.11 (from -r requirements.txt (line 21))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting MarkupSafe==1.0 (from -r requirements.txt (line 22))\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz\n",
      "Collecting matplotlib==2.1.2 (from -r requirements.txt (line 23))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/bd/3e8cec37bcf71cfd81fe798cf733c046b1ceb123e7dddf6d3435cf03b506/matplotlib-2.1.2.tar.gz (36.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 36.2MB 1.1MB/s ta 0:00:011    68% |█████████████████████▉          | 24.7MB 7.3MB/s eta 0:00:02\n",
      "\u001b[?25h    Complete output from command python setup.py egg_info:\n",
      "    IMPORTANT WARNING:\n",
      "        pkg-config is not installed.\n",
      "        matplotlib may not be able to find some of its dependencies\n",
      "    ============================================================================\n",
      "    Edit setup.cfg to change the build options\n",
      "    \n",
      "    BUILDING MATPLOTLIB\n",
      "                matplotlib: yes [2.1.2]\n",
      "                    python: yes [3.7.2 (default, Dec 29 2018, 00:00:04)  [Clang\n",
      "                            4.0.1 (tags/RELEASE_401/final)]]\n",
      "                  platform: yes [darwin]\n",
      "    \n",
      "    REQUIRED DEPENDENCIES AND EXTENSIONS\n",
      "                     numpy: yes [version 1.15.4]\n",
      "                       six: yes [using six version 1.12.0]\n",
      "                  dateutil: yes [using dateutil version 2.8.0]\n",
      "    backports.functools_lru_cache: yes [Not required]\n",
      "              subprocess32: yes [Not required]\n",
      "                      pytz: yes [using pytz version 2018.9]\n",
      "                    cycler: yes [using cycler version 0.10.0]\n",
      "                   tornado: yes [using tornado version 5.1.1]\n",
      "                 pyparsing: yes [using pyparsing version 2.3.1]\n",
      "                    libagg: yes [pkg-config information for 'libagg' could not\n",
      "                            be found. Using local copy.]\n",
      "                  freetype: no  [The C/C++ header for freetype2 (ft2build.h)\n",
      "                            could not be found.  You may need to install the\n",
      "                            development package.]\n",
      "                       png: yes [version 1.6.36]\n",
      "                     qhull: yes [pkg-config information for 'libqhull' could not\n",
      "                            be found. Using local copy.]\n",
      "    \n",
      "    OPTIONAL SUBPACKAGES\n",
      "               sample_data: yes [installing]\n",
      "                  toolkits: yes [installing]\n",
      "                     tests: no  [skipping due to configuration]\n",
      "            toolkits_tests: no  [skipping due to configuration]\n",
      "    \n",
      "    OPTIONAL BACKEND EXTENSIONS\n",
      "                    macosx: yes [installing, darwin]\n",
      "                    qt5agg: yes [installing, Qt: 5.6.2, PyQt: 5.6.2; PySide2 not\n",
      "                            found]\n",
      "                    qt4agg: no  [PySide not found; PyQt4 not found]\n",
      "                   gtk3agg: no  [Requires pygobject to be installed.]\n",
      "                 gtk3cairo: no  [Requires cairocffi or pycairo to be installed.]\n",
      "                    gtkagg: no  [Requires pygtk]\n",
      "                     tkagg: yes [installing; run-time loading from Python Tcl /\n",
      "                            Tk]\n",
      "                     wxagg: yes [installing, version 4.0.3]\n",
      "                       gtk: no  [Requires pygtk]\n",
      "                       agg: yes [installing]\n",
      "                     cairo: no  [cairocffi or pycairo not found]\n",
      "                 windowing: no  [Microsoft Windows only]\n",
      "    \n",
      "    OPTIONAL LATEX DEPENDENCIES\n",
      "                    dvipng: no\n",
      "               ghostscript: no\n",
      "                     latex: no\n",
      "                   pdftops: no\n",
      "    \n",
      "    OPTIONAL PACKAGE DATA\n",
      "                      dlls: no  [skipping due to configuration]\n",
      "    \n",
      "    ============================================================================\n",
      "                            * The following required packages can not be built:\n",
      "                            * freetype\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-ra9t1mk9/matplotlib/\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0YcTuo3MmBS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting request\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/27/7cbde262d854aedf217061a97020d66a63163c5c04e0ec02ff98c5d8f44e/request-2019.4.13.tar.gz\n",
      "Collecting get (from request)\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/ef/bb46f77f7220ac1b7edba0c76d810c89fddb24ddd8c08f337b9b4a618db7/get-2019.4.13.tar.gz\n",
      "Collecting post (from request)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/05/bd79da5849ea6a92485ed7029ef97b1b75e55c26bc0ed3a7ec769af666f3/post-2019.4.13.tar.gz\n",
      "Requirement already satisfied: setuptools in /Users/ashishsingh/anaconda3/lib/python3.7/site-packages (from request) (40.8.0)\n",
      "Collecting query_string (from get->request)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/3c/412a45daf5bea9b1d06d7de41787ec4168001dfa418db7ec8723356b119f/query-string-2019.4.13.tar.gz\n",
      "Collecting public (from query_string->get->request)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/4d/b40004cc6c07665e48af22cfe1e631f219bf4282e15fa76a5b6364f6885c/public-2019.4.13.tar.gz\n",
      "Building wheels for collected packages: request, get, post, query-string, public\n",
      "  Building wheel for request (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashishsingh/Library/Caches/pip/wheels/30/84/5f/484cfba678967ef58c16fce6890925d5c7172622f20111fbfd\n",
      "  Building wheel for get (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashishsingh/Library/Caches/pip/wheels/c1/e3/c1/d02c8c58538853e4c9b78cadb74f6d5c5c370b48a69a7271aa\n",
      "  Building wheel for post (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashishsingh/Library/Caches/pip/wheels/c3/c3/24/b5c132b537ab380c02d69e6bd4dec1f5db56b5fe19030473d7\n",
      "  Building wheel for query-string (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashishsingh/Library/Caches/pip/wheels/d6/a4/78/01b20a9dc224dcc009fab669f7f27b943b8889c5150bd68d8a\n",
      "  Building wheel for public (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ashishsingh/Library/Caches/pip/wheels/23/7c/6e/f5b4e09d6596c8b8802b347e48f149031e2363368048f1347a\n",
      "Successfully built request get post query-string public\n",
      "Installing collected packages: public, query-string, get, post, request\n",
      "Successfully installed get-2019.4.13 post-2019.4.13 public-2019.4.13 query-string-2019.4.13 request-2019.4.13\n"
     ]
    }
   ],
   "source": [
    "!pip install request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aI_DPyNaeaX"
   },
   "source": [
    "### Installing Dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to locate an executable at \"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/bin/apt\" (-1)\r\n"
     ]
    }
   ],
   "source": [
    "!apt install python python-pip build-essential cmake pkg-config libx11-dev libatlas-base-dev libgtk-3-dev libboost-python-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "!sudo install python python-pip build-essential cmake pkg-config libx11-dev libatlas-base-dev libgtk-3-dev libboost-python-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uZqN-rFXknw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/e8a8caa3c89a27f80bc78da39c423e2553f482a3705adc619176a3a24b36/dlib-19.17.0.tar.gz\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py) ... \u001b[?25lerror\n",
      "  Complete output from command /Users/ashishsingh/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-wheel-0_gkbyw6 --python-tag cp37:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  package init file 'dlib/__init__.py' not found (or not a regular file)\n",
      "  running build_ext\n",
      "  Traceback (most recent call last):\n",
      "    File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 120, in get_cmake_version\n",
      "      out = subprocess.check_output(['cmake', '--version'])\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 395, in check_output\n",
      "      **kwargs).stdout\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 472, in run\n",
      "      with Popen(*popenargs, **kwargs) as process:\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "      restore_signals, start_new_session)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "      raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "  FileNotFoundError: [Errno 2] No such file or directory: 'cmake': 'cmake'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 261, in <module>\n",
      "      'Topic :: Software Development',\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 192, in run\n",
      "      self.run_command('build')\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 129, in run\n",
      "      cmake_version = self.get_cmake_version()\n",
      "    File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 125, in get_cmake_version\n",
      "      \"\\n*******************************************************************\\n\")\n",
      "  RuntimeError:\n",
      "  *******************************************************************\n",
      "   CMake must be installed to build the following extensions: dlib\n",
      "  *******************************************************************\n",
      "  \n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for dlib\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for dlib\n",
      "Failed to build dlib\n",
      "Installing collected packages: dlib\n",
      "  Running setup.py install for dlib ... \u001b[?25lerror\n",
      "    Complete output from command /Users/ashishsingh/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-record-gf9aoaqm/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    package init file 'dlib/__init__.py' not found (or not a regular file)\n",
      "    running build_ext\n",
      "    Traceback (most recent call last):\n",
      "      File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 120, in get_cmake_version\n",
      "        out = subprocess.check_output(['cmake', '--version'])\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 395, in check_output\n",
      "        **kwargs).stdout\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 472, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "        restore_signals, start_new_session)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "        raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'cmake': 'cmake'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 261, in <module>\n",
      "        'Topic :: Software Development',\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/site-packages/setuptools/command/install.py\", line 61, in run\n",
      "        return orig.install.run(self)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/command/install.py\", line 545, in run\n",
      "        self.run_command('build')\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/command/build.py\", line 135, in run\n",
      "        self.run_command(cmd_name)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/Users/ashishsingh/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 129, in run\n",
      "        cmake_version = self.get_cmake_version()\n",
      "      File \"/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py\", line 125, in get_cmake_version\n",
      "        \"\\n*******************************************************************\\n\")\n",
      "    RuntimeError:\n",
      "    *******************************************************************\n",
      "     CMake must be installed to build the following extensions: dlib\n",
      "    *******************************************************************\n",
      "    \n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCommand \"/Users/ashishsingh/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-record-gf9aoaqm/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/kd/z3hjh7wn417074v_nblp7myh0000gn/T/pip-install-vrvrrur3/dlib/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXiNriJXOYCa"
   },
   "source": [
    "### Download Dlib's face landmarks data file for running face alignment.\n",
    "\n",
    "This will helps us in aligning faces before we learn the features for each face. **`Run the below code.`** It will create a directory with name **`models` **and save **`landmarks.dat`** file in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOAofB2nLoii"
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import os\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def download_landmarks(dst_file):\n",
    "    url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    with urlopen(url) as src, open(dst_file, 'wb') as dst:\n",
    "        data = src.read(1024)\n",
    "        while len(data) > 0:\n",
    "            dst.write(decompressor.decompress(data))\n",
    "            data = src.read(1024)\n",
    "\n",
    "dst_dir = 'models'\n",
    "dst_file = os.path.join(dst_dir, 'landmarks.dat')\n",
    "\n",
    "if not os.path.exists(dst_file):\n",
    "    os.makedirs(dst_dir)\n",
    "    download_landmarks(dst_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNbjzAgh3em-"
   },
   "source": [
    "### Training the network\n",
    "\n",
    "The CNN model is taken from the Keras-OpenFace project. The architecture details aren't too important here, it's only useful to know that there is a fully connected layer with 128 hidden units followed by an L2 normalization layer on top of the convolutional base. These two top layers are referred to as the embedding layer from which the 128-dimensional embedding vectors can be obtained. The complete model is defined in `model.py` and a graphical overview is given in `model.png`. A Keras version of the `nn4.small2` model can be created with `create_model()`.\n",
    "\n",
    "\n",
    "**Run the below code to initialize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRMdAbSsMcPC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ashishsingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from model import create_model\n",
    "\n",
    "nn4_small2 = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyCaIac9N8_8"
   },
   "source": [
    "#### Idea of Training the model with Triplet loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNKudbMe4u7W"
   },
   "source": [
    "Model training aims to learn an embedding f(x) of image x such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a triplet loss L that is minimized when the distance between an anchor image xai and a positive image xpi (same identity) in embedding space is smaller than the distance between that anchor image and a negative image xni (different identity) by at least a margin α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ose4tTyPDeU"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(96, 96, 3))\n",
    "in_p = Input(shape=(96, 96, 3))\n",
    "in_n = Input(shape=(96, 96, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpTO6eimP5t0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ashishsingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 0.7999\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 0.7961\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 0.8003\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 0.8003\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 58s 576ms/step - loss: 0.8001\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 0.8002\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 53s 533ms/step - loss: 0.7994\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 0.8087\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 57s 575ms/step - loss: 0.8000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 0.7999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb308dbef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import triplet_generator\n",
    "\n",
    "# triplet_generator() creates a generator that continuously returns \n",
    "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
    "# and n_batch are batches of anchor, positive and negative RGB images \n",
    "# each having a shape of (batch_size, 96, 96, 3).\n",
    "generator = triplet_generator() \n",
    "\n",
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
    "\n",
    "# Please note that the current implementation of the generator only generates \n",
    "# random image data. The main goal of this code snippet is to demonstrate \n",
    "# the general setup for model training. In the following, we will anyway \n",
    "# use a pre-trained model so we don't need a generator here that operates \n",
    "# on real training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdsuNBe9OHyZ"
   },
   "source": [
    "For this project, we are considering a pre-trained model given in file path **`nn4.small2.v1.h5`**.\n",
    "\n",
    "Write code: Using **load_weights()** function load the given pre-trained weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oV73nKP3QCLT"
   },
   "outputs": [],
   "source": [
    "nn4_small2_train.load_weights(\"nn4.small2.v1.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bulEElwQ54g6"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re9AUK4f520z"
   },
   "source": [
    "To demonstrate face recognition on a custom dataset, a small dataset is used. It consists of around 15-25 face images of 10 different persons. The metadata for each image (file and identity name) are loaded into memory for later processing.\n",
    "\n",
    "\n",
    "Upload Images zip file given to drive and download and extract it using the below code. And we will pass the folder `images` to `load_metadata` function to save all the images filenames and person numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbDurWRMQnw7"
   },
   "source": [
    "#### Import drive module from google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYr8qfMD7Poc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GlBCA8wQ1cX"
   },
   "source": [
    "#### Give a path to mount the files in your drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxhxrGQS7Pq2"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBB_OncAQ8h_"
   },
   "source": [
    "#### Using the above given mounted path, give the images.zip path dependent on where you placed the file in your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pprDxdHT7S6N"
   },
   "outputs": [],
   "source": [
    "## For example\n",
    "# images_path = \"/content/drive/My Drive/DLCP/Project-2/images.zip\"\n",
    "images_path = proj_path + \"images.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edsF05iuRkOd"
   },
   "source": [
    "#### Using ZipFile module to extract the images zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3eIglFdVcvB"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVAjzG4IXYQX"
   },
   "outputs": [],
   "source": [
    "with ZipFile(images_path, 'r') as zip:\n",
    "  zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oesXJD9ySB6w"
   },
   "source": [
    "#### Run the below function to load the images from the extracted images folder from the above step and map each image with person id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Q7TS19vVbGb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # print(base, name, file)\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "            # Check file extension. Allow only jpg/jpeg' files.\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext == '.jpg' or ext == '.jpeg':\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vr4xVNqIaHgE"
   },
   "source": [
    "### Face alignment\n",
    "The nn4.small2.v1 model was trained with aligned face images, therefore, the face images from the custom dataset must be aligned too. Here, we use Dlib for face detection and OpenCV for image transformation and cropping to produce aligned 96x96 RGB face images. We are using the AlignDlib utility from the OpenFace project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZJXEFz9UaAR"
   },
   "source": [
    "##### 1. Run the below code to import AlignDlib\n",
    "\n",
    "For this you need align.py available in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFCOKB1AUq0c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0acaca2b6daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malign\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AIML/Assignments/Project 2 | Advance Computer Vision/align.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "from align import AlignDlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsTnKz_zWVM_"
   },
   "source": [
    "Use the landmarks data file downloaded in the first steps for face alignment. file path **`models/landmarks.dat`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijh2N8QxWSN4"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbWhL7jbUwHg"
   },
   "source": [
    "##### 2. Run the beloiw code to  load an image from the metadata created in the step before Face Alignment\n",
    "\n",
    "You can access each image path from `metadata[i].image_path()` where, i is the image number. i can take values from 1 to no.of images in the dataset given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ape5WxvVWKOe"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptDNq8noWK89"
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "# for example, loading the image with index 1\n",
    "one_image = load_image(metadata[0].image_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpe2G6JSbTis"
   },
   "source": [
    "#### Write code to load 2nd and 3rd images in the metadata using load_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEEzp1SkbdxI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0ywYChhXHQI"
   },
   "source": [
    "##### 3. Run the below code to align the above loaded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_GO6xugXHd8"
   },
   "outputs": [],
   "source": [
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(one_image)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "one_image_aligned = alignment.align(96, one_image, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(131)\n",
    "plt.imshow(one_image)\n",
    "\n",
    "# Show original image with bounding box\n",
    "plt.subplot(132)\n",
    "plt.imshow(one_image)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "\n",
    "# Show aligned image\n",
    "plt.subplot(133)\n",
    "plt.imshow(one_image_aligned);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5BrN6vpX_kj"
   },
   "source": [
    "#### Write a function image_align() which take image path as input and returns the aligned image in output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inrLd5JCYYB2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkBQRL_sd2U8"
   },
   "source": [
    "### Generate embeddings for each image in the dataset\n",
    "\n",
    "Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S01r8UzXc-8s"
   },
   "source": [
    "#### Get embedding vector for first image in the metadata using the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2yd69OydBAq"
   },
   "outputs": [],
   "source": [
    "# Align the image\n",
    "img_aligned = image_align(metadata[0].image_path())\n",
    "\n",
    "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
    "img = (img_aligned / 255.).astype(np.float32)\n",
    "\n",
    "# obtain embedding vector for an image\n",
    "embedding_vector = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plHvUTytcTGo"
   },
   "source": [
    "#### Write code to iterate through metadata and create embeddings for each image using nn4_small2_pretrained.predict() and store in a list with name `embeddings`\n",
    "\n",
    "If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 128-zeroes as the final embedding from the model is of length 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY9ykxtueY4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hb3XSDsfTMG"
   },
   "source": [
    "#### Write code to get the distance between given 2 pairs of images.\n",
    "\n",
    "Consider distance metric as \"Squared L2 distance\"\n",
    "\n",
    "squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2\n",
    "\n",
    "\n",
    "\n",
    "##### Plot images and get distance between the pairs given below.\n",
    "\n",
    "1. 2,3 and 2,120\n",
    "\n",
    "2. 30,31 and 30,100\n",
    "\n",
    "3. 70,72 and 70,115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KF1OFzRDsP--"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dh1MxMTzsQNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMv9vqACsQ12"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_HfgsQXgV6u"
   },
   "source": [
    "#### Now lets build a SVM classifier to predict person in the given image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AahHoLK1hZJj"
   },
   "source": [
    "Use LinearSVC in sklearn.svm\n",
    "\n",
    "Run the below code to divide half of the images as training set and another half of the images as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gfLzoBIhrpV"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "## checking the shapes of metaadata and test and train sets\n",
    "print(metadata.shape)\n",
    "print(train_idx.shape)\n",
    "print(test_idx.shape)\n",
    "\n",
    "\n",
    "# one half as train examples of 10 identities\n",
    "X_train = embedding[train_idx]\n",
    "# another half as test examples of 10 identities\n",
    "X_test = embedding[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "312NcUeuiNpL"
   },
   "source": [
    "#### Build SVM and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ybglrDph7tJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqFfYIZbiXG8"
   },
   "source": [
    "#### Test the classifier\n",
    "\n",
    "Take 35th image from test set and plot the image, report to which person(folder name in dataset) the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2E6WBuMiBmX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FACE RECOGNITION_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
