{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ashishsingh/AIML/Assignments/Introduction to Neural Networks and Deep Learning_R7_INN_Project2/Introduction to Neural Networks and Deep Learning_R7_INN_Project2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() #to get current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = 'SVHN_single_grey1.h5'\n",
    "dataFrame = h5py.File(filename, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']>\n"
     ]
    }
   ],
   "source": [
    "# # List all groups\n",
    "print(\"Keys: %s\" % dataFrame.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetnames = [n for n in dataFrame.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test\n",
      "X_train\n",
      "X_val\n",
      "y_test\n",
      "y_train\n",
      "y_val\n"
     ]
    }
   ],
   "source": [
    "for n in datasetnames:\n",
    "    print(n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = dataFrame['X_test']\n",
    "x_train = dataFrame['X_train']\n",
    "x_val = dataFrame['X_val']\n",
    "y_test = dataFrame['y_test']\n",
    "y_train = dataFrame['y_train']\n",
    "y_val = dataFrame['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"X_test\": shape (18000, 32, 32), type \"<f4\">\n",
      "<HDF5 dataset \"X_train\": shape (42000, 32, 32), type \"<f4\">\n",
      "<HDF5 dataset \"X_val\": shape (60000, 32, 32), type \"<f4\">\n",
      "<HDF5 dataset \"y_test\": shape (18000,), type \"|u1\">\n",
      "<HDF5 dataset \"y_train\": shape (42000,), type \"|u1\">\n",
      "<HDF5 dataset \"y_val\": shape (60000,), type \"|u1\">\n"
     ]
    }
   ],
   "source": [
    "for i in dataFrame.keys():\n",
    "    print(dataFrame[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"X_train\": shape (42000, 32, 32), type \"<f4\">"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dataFrame['X_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dataFrame['X_train'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.0704,  30.2601,  26.852 , ...,  71.4471,  58.2204,  42.9939],\n",
       "       [ 25.2283,  25.5533,  29.9765, ..., 113.0209, 103.3639,  84.2949],\n",
       "       [ 26.2775,  22.6137,  40.4763, ..., 113.3028, 121.775 , 115.4228],\n",
       "       ...,\n",
       "       [ 28.5502,  36.212 ,  45.0801, ...,  24.1359,  25.0927,  26.0603],\n",
       "       [ 38.4352,  26.4733,  23.2717, ...,  28.1094,  29.4683,  30.0661],\n",
       "       [ 50.2984,  26.0773,  24.0389, ...,  49.6682,  50.853 ,  53.0377]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataFrame['X_train'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement and apply an optimal k-Nearest Neighbor (kNN) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv2.resize(list(dataFrame['X_train'])[0], (32,32)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening from 3D to 2D\n",
    "X_train = np.reshape(x_train,(42000,32*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNH = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNH.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "# Change the dimension from 3D -> 2D\n",
    "X_test = np.reshape(x_test,(18000,32*32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = NNH.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5168333333333334\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "print(\"Accuracy:\", metrics.accuracy_score(prediction,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[1240   94  110  134  119  160  304  102  257  343]\n",
      " [  62 1344  236  265  246  173  118  239  124  144]\n",
      " [  47   64  991  145   57   69   44  134   72   73]\n",
      " [  38   85  113  719   58  288   75   83  132  107]\n",
      " [  47   79   45   46 1154   71  132   32   93   74]\n",
      " [  46   39   31  161   26  671  137   29  124   94]\n",
      " [ 111   31   30   48   57  135  743   30  254   63]\n",
      " [  26   41  133   53   15   23   23 1090   22   65]\n",
      " [  91   21   46   90   39  107  201   28  636  126]\n",
      " [ 106   30   68   58   41   71   55   41   98  715]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "print(\"Confusion Matrix:\", metrics.confusion_matrix(prediction,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.68      0.53      1814\n",
      "           1       0.46      0.74      0.56      1828\n",
      "           2       0.58      0.55      0.57      1803\n",
      "           3       0.42      0.42      0.42      1719\n",
      "           4       0.65      0.64      0.64      1812\n",
      "           5       0.49      0.38      0.43      1768\n",
      "           6       0.49      0.41      0.45      1832\n",
      "           7       0.73      0.60      0.66      1808\n",
      "           8       0.46      0.35      0.40      1812\n",
      "           9       0.56      0.40      0.46      1804\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     18000\n",
      "   macro avg       0.53      0.52      0.51     18000\n",
      "weighted avg       0.53      0.52      0.51     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "cr=metrics.classification_report(y_test,prediction)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement and apply a deep neural network classifier including (feedforward neural network, RELU activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape data from 2D to 1D -> 32X32 to 1024\n",
    "model.add(tf.keras.layers.Reshape((1024,),input_shape=(32,32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization for training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ashishsingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change train and test labels into one-hot vectors\n",
    "trainY = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "testY = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "WARNING:tensorflow:From /Users/ashishsingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/75\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 2.1309 - acc: 0.2624 - val_loss: 1.8501 - val_acc: 0.4179\n",
      "Epoch 2/75\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 1.6557 - acc: 0.4990 - val_loss: 1.4562 - val_acc: 0.5754\n",
      "Epoch 3/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 1.3465 - acc: 0.6068 - val_loss: 1.2351 - val_acc: 0.6379\n",
      "Epoch 4/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 1.1752 - acc: 0.6564 - val_loss: 1.1119 - val_acc: 0.6785\n",
      "Epoch 5/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 1.0700 - acc: 0.6886 - val_loss: 1.0350 - val_acc: 0.6965\n",
      "Epoch 6/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.9965 - acc: 0.7075 - val_loss: 0.9701 - val_acc: 0.7176\n",
      "Epoch 7/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.9381 - acc: 0.7255 - val_loss: 0.9274 - val_acc: 0.7290\n",
      "Epoch 8/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.8877 - acc: 0.7410 - val_loss: 0.8920 - val_acc: 0.7408\n",
      "Epoch 9/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.8454 - acc: 0.7514 - val_loss: 0.8565 - val_acc: 0.7502\n",
      "Epoch 10/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.8069 - acc: 0.7640 - val_loss: 0.8345 - val_acc: 0.7563\n",
      "Epoch 11/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.7735 - acc: 0.7727 - val_loss: 0.8009 - val_acc: 0.7681\n",
      "Epoch 12/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.7420 - acc: 0.7819 - val_loss: 0.7819 - val_acc: 0.7752\n",
      "Epoch 13/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.7158 - acc: 0.7903 - val_loss: 0.7649 - val_acc: 0.7809\n",
      "Epoch 14/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.6916 - acc: 0.7955 - val_loss: 0.7414 - val_acc: 0.7874\n",
      "Epoch 15/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.6683 - acc: 0.8040 - val_loss: 0.7390 - val_acc: 0.7875\n",
      "Epoch 16/75\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6483 - acc: 0.8092 - val_loss: 0.7141 - val_acc: 0.7965\n",
      "Epoch 17/75\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6291 - acc: 0.8154 - val_loss: 0.7002 - val_acc: 0.8015\n",
      "Epoch 18/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.6110 - acc: 0.8209 - val_loss: 0.6917 - val_acc: 0.8032\n",
      "Epoch 19/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5951 - acc: 0.8245 - val_loss: 0.6760 - val_acc: 0.8094\n",
      "Epoch 20/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.5794 - acc: 0.8299 - val_loss: 0.6678 - val_acc: 0.8115\n",
      "Epoch 21/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5660 - acc: 0.8342 - val_loss: 0.6614 - val_acc: 0.8127\n",
      "Epoch 22/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5526 - acc: 0.8386 - val_loss: 0.6522 - val_acc: 0.8152\n",
      "Epoch 23/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.5400 - acc: 0.8417 - val_loss: 0.6496 - val_acc: 0.8187\n",
      "Epoch 24/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5291 - acc: 0.8454 - val_loss: 0.6467 - val_acc: 0.8164\n",
      "Epoch 25/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5179 - acc: 0.8485 - val_loss: 0.6352 - val_acc: 0.8224\n",
      "Epoch 26/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.5087 - acc: 0.8500 - val_loss: 0.6414 - val_acc: 0.8204\n",
      "Epoch 27/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.4985 - acc: 0.8535 - val_loss: 0.6270 - val_acc: 0.8247\n",
      "Epoch 28/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.4880 - acc: 0.8570 - val_loss: 0.6266 - val_acc: 0.8239\n",
      "Epoch 29/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.4789 - acc: 0.8602 - val_loss: 0.6172 - val_acc: 0.8280\n",
      "Epoch 30/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.4717 - acc: 0.8609 - val_loss: 0.6448 - val_acc: 0.8152\n",
      "Epoch 31/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.4631 - acc: 0.8648 - val_loss: 0.6107 - val_acc: 0.8282\n",
      "Epoch 32/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.4550 - acc: 0.8676 - val_loss: 0.6062 - val_acc: 0.8311\n",
      "Epoch 33/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.4490 - acc: 0.8680 - val_loss: 0.5985 - val_acc: 0.8342\n",
      "Epoch 34/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4427 - acc: 0.8705 - val_loss: 0.6118 - val_acc: 0.8286\n",
      "Epoch 35/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4353 - acc: 0.8723 - val_loss: 0.6027 - val_acc: 0.8303\n",
      "Epoch 36/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4279 - acc: 0.8753 - val_loss: 0.6041 - val_acc: 0.8323\n",
      "Epoch 37/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4209 - acc: 0.8774 - val_loss: 0.5947 - val_acc: 0.8343\n",
      "Epoch 38/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4166 - acc: 0.8780 - val_loss: 0.5901 - val_acc: 0.8374\n",
      "Epoch 39/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4096 - acc: 0.8798 - val_loss: 0.5904 - val_acc: 0.8375\n",
      "Epoch 40/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.4033 - acc: 0.8804 - val_loss: 0.6069 - val_acc: 0.8307\n",
      "Epoch 41/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3980 - acc: 0.8840 - val_loss: 0.6065 - val_acc: 0.8341\n",
      "Epoch 42/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3938 - acc: 0.8845 - val_loss: 0.5839 - val_acc: 0.8404\n",
      "Epoch 43/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3871 - acc: 0.8864 - val_loss: 0.5898 - val_acc: 0.8369\n",
      "Epoch 44/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3827 - acc: 0.8875 - val_loss: 0.5902 - val_acc: 0.8376\n",
      "Epoch 45/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3771 - acc: 0.8899 - val_loss: 0.5999 - val_acc: 0.8349\n",
      "Epoch 46/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.3720 - acc: 0.8905 - val_loss: 0.5877 - val_acc: 0.8384\n",
      "Epoch 47/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3685 - acc: 0.8920 - val_loss: 0.6090 - val_acc: 0.8322\n",
      "Epoch 48/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3634 - acc: 0.8928 - val_loss: 0.5860 - val_acc: 0.8400\n",
      "Epoch 49/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.3627 - acc: 0.8934 - val_loss: 0.5794 - val_acc: 0.8433\n",
      "Epoch 50/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.3528 - acc: 0.8958 - val_loss: 0.5990 - val_acc: 0.8371\n",
      "Epoch 51/75\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3527 - acc: 0.8973 - val_loss: 0.5863 - val_acc: 0.8403\n",
      "Epoch 52/75\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.3460 - acc: 0.8981 - val_loss: 0.5998 - val_acc: 0.8379\n",
      "Epoch 53/75\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3421 - acc: 0.8987 - val_loss: 0.5789 - val_acc: 0.8433\n",
      "Epoch 54/75\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3402 - acc: 0.8996 - val_loss: 0.5893 - val_acc: 0.8405\n",
      "Epoch 55/75\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.3354 - acc: 0.9018 - val_loss: 0.5960 - val_acc: 0.8398\n",
      "Epoch 56/75\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.3307 - acc: 0.9030 - val_loss: 0.5975 - val_acc: 0.8372\n",
      "Epoch 57/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3270 - acc: 0.9050 - val_loss: 0.5895 - val_acc: 0.8398\n",
      "Epoch 58/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3235 - acc: 0.9055 - val_loss: 0.5850 - val_acc: 0.8438\n",
      "Epoch 59/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3195 - acc: 0.9065 - val_loss: 0.6062 - val_acc: 0.8366\n",
      "Epoch 60/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3174 - acc: 0.9069 - val_loss: 0.6129 - val_acc: 0.8367\n",
      "Epoch 61/75\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3159 - acc: 0.9070 - val_loss: 0.5924 - val_acc: 0.8420\n",
      "Epoch 62/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3086 - acc: 0.9099 - val_loss: 0.6035 - val_acc: 0.8404\n",
      "Epoch 63/75\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.3078 - acc: 0.9110 - val_loss: 0.5867 - val_acc: 0.8453\n",
      "Epoch 64/75\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 0.3020 - acc: 0.9113 - val_loss: 0.5947 - val_acc: 0.8429\n",
      "Epoch 65/75\n",
      "42000/42000 [==============================] - 3s 69us/sample - loss: 0.3016 - acc: 0.9117 - val_loss: 0.5894 - val_acc: 0.8437\n",
      "Epoch 66/75\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 0.2974 - acc: 0.9117 - val_loss: 0.5885 - val_acc: 0.8448\n",
      "Epoch 67/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2936 - acc: 0.9143 - val_loss: 0.6168 - val_acc: 0.8357\n",
      "Epoch 68/75\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.2899 - acc: 0.9157 - val_loss: 0.5904 - val_acc: 0.8442\n",
      "Epoch 69/75\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.2868 - acc: 0.9162 - val_loss: 0.6012 - val_acc: 0.8426\n",
      "Epoch 70/75\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.2860 - acc: 0.9162 - val_loss: 0.6193 - val_acc: 0.8398\n",
      "Epoch 71/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2838 - acc: 0.9174 - val_loss: 0.6098 - val_acc: 0.8424\n",
      "Epoch 72/75\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.2841 - acc: 0.9158 - val_loss: 0.5960 - val_acc: 0.8438\n",
      "Epoch 73/75\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.2792 - acc: 0.9191 - val_loss: 0.5990 - val_acc: 0.8438\n",
      "Epoch 74/75\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2756 - acc: 0.9191 - val_loss: 0.5940 - val_acc: 0.8460\n",
      "Epoch 75/75\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2724 - acc: 0.9198 - val_loss: 0.5871 - val_acc: 0.8493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a28b2aeb8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "features_val_arr = np.array(x_test)\n",
    "model.fit(x_train,trainY,          \n",
    "          validation_data=(features_val_arr, testY),\n",
    "          epochs=75,\n",
    "          batch_size=150,validation_split = 0.01,\n",
    "         shuffle='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 56us/sample - loss: 0.5871 - acc: 0.8493\n",
      "[0.5871379801829656, 0.84933335]\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "evaluate = model.evaluate(x_test, testY)\n",
    "print(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "y_predict=model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      1814\n",
      "           1       0.85      0.86      0.85      1828\n",
      "           2       0.88      0.86      0.87      1803\n",
      "           3       0.77      0.83      0.80      1719\n",
      "           4       0.89      0.88      0.89      1812\n",
      "           5       0.83      0.83      0.83      1768\n",
      "           6       0.85      0.83      0.84      1832\n",
      "           7       0.88      0.88      0.88      1808\n",
      "           8       0.83      0.81      0.82      1812\n",
      "           9       0.84      0.83      0.83      1804\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     18000\n",
      "   macro avg       0.85      0.85      0.85     18000\n",
      "weighted avg       0.85      0.85      0.85     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr=metrics.classification_report(y_test,y_predict)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_to_feature_vector(image, size=(32, 32)):\n",
    "# \t# resize the image to a fixed size, then flatten the image into\n",
    "# \t# a list of raw pixel intensities\n",
    "# \treturn cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train and evaluate a k-NN classifer on the histogram\n",
    "# # representations\n",
    "# print(\"[INFO] evaluating accuracy...\")\n",
    "\n",
    "# NNH.fit(X_train_new, y_train)\n",
    "# acc = NNH.score(X_train_new, y_train)\n",
    "# print(\"[INFO]  accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification metrics report\n",
    "# sklearn.metrics.classification_report(y_test, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K fold\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # creating odd list of K for KNN\n",
    "# myList = list(range(1,50))\n",
    "\n",
    "\n",
    "# # empty list that will hold cv scores\n",
    "# cv_scores = []\n",
    "# k_neighbors = []\n",
    "\n",
    "# # perform 10-fold cross validation\n",
    "# for k in myList:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(knn, X_train_new, y_train, cv=10, scoring='accuracy')\n",
    "#     cv_scores.append(scores.mean())\n",
    "#     k_neighbors.append(k)\n",
    "\n",
    "\n",
    "# MSE = [1 - x for x in cv_scores]\n",
    "# min(MSE)\n",
    "# MSE.index(min(MSE))\n",
    "# best_k = myList[MSE.index(min(MSE))]\n",
    "# print (\"The optimal number of neighbors is %d\" % best_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
